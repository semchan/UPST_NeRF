
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

<script src="http://www.google.com/jsapi" type="text/javascript"></script>
<script type="text/javascript">google.load("jquery", "1.3.2");</script>

<style type="text/css">
body {
    font-family: "Titillium Web", "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
    font-weight: 300;
    font-size: 17px;
    margin-left: auto;
    margin-right: auto;
    width: 980px;
}
h1 {
    font-weight:300;
    line-height: 1.15em;
}

h2 {
    font-size: 1.75em;
}
a:link,a:visited {
    color: #1367a7;
    text-decoration: none;
}
a:hover {
    color: #208799;
}
h1, h2, h3 {
    text-align: center;
}
h1 {
    font-size: 40px;
    font-weight: 500;
}
h2 {
    font-weight: 400;
    margin: 16px 0px 4px 0px;
}
.paper-title {
    padding: 16px 0px 16px 0px;
}
section {
    margin: 32px 0px 32px 0px;
    text-align: justify;
    clear: both;
}
.col-6 {
     width: 16.6%;
     float: left;
}
.col-5 {
     width: 20%;
     float: left;
}
.col-4 {
     width: 25%;
     float: left;
}
.col-3 {
     width: 33%;
     float: left;
}
.col-2 {
     width: 50%;
     float: left;
}
.row, .author-row, .affil-row {
     overflow: auto;
}
.author-row, .affil-row {
    font-size: 20px;
}
.row {
    margin: 16px 0px 16px 0px;
}
.authors {
    font-size: 18px;
}
.affil-row {
    margin-top: 16px;
}
.teaser {
    max-width: 100%;
}
.text-center {
    text-align: center;  
}
.screenshot {
    width: 256px;
    border: 1px solid #ddd;
}
.screenshot-el {
    margin-bottom: 16px;
}
hr {
    height: 1px;
    border: 0; 
    border-top: 1px solid #ddd;
    margin: 0;
}
.material-icons {
    vertical-align: -6px;
}
p {
    line-height: 1.25em;
}
.caption {
    font-size: 16px;
    /*font-style: italic;*/
    color: #666;
    text-align: left;
    margin-top: 8px;
    margin-bottom: 8px;
}
video {
    display: block;
    margin: auto;
}
figure {
    display: block;
    margin: auto;
    margin-top: 10px;
    margin-bottom: 10px;
}
#bibtex pre {
    font-size: 14px;
    background-color: #eee;
    padding: 16px;
}
.blue {
    color: #2c82c9;
    font-weight: bold;
}
.orange {
    color: #d35400;
    font-weight: bold;
}
.flex-row {
    display: flex;
    flex-flow: row wrap;
    justify-content: space-around;
    padding: 0;
    margin: 0;
    list-style: none;
}
.paper-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 100px;
  font-weight: 600;
}

.supp-btn {
  position: relative;
  text-align: center;

  display: inline-block;
  margin: 8px;
  padding: 8px 8px;

  border-width: 0;
  outline: none;
  border-radius: 2px;
  
  background-color: #1367a7;
  color: #ecf0f1 !important;
  font-size: 20px;
  width: 150px;
  font-weight: 600;
}

.paper-btn-parent {
    display: flex;
    justify-content: center;
    margin: 16px 0px;
}
.paper-btn:hover {
    opacity: 0.85;
}
.container {
    margin-left: auto;
    margin-right: auto;
    padding-left: 16px;
    padding-right: 16px;
}
.venue {
    color: #1367a7;
}



.topnav {
  overflow: hidden;
  background-color: #EEEEEE;
}

.topnav a {
  float: left;
  color: black;
  text-align: center;
  padding: 14px 16px;
  text-decoration: none;
  font-size: 16px;
}



</style>


<div class="topnav" id="myTopnav">

</div>


<!-- End : Google Analytics Code -->
<script type="text/javascript" src="../js/hidebib.js"></script>
<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
<head>
    <title>UPST-NeRF: Universal Photorealistic Style Transfer of Neural Radiance Fields for 3D Scene</title>
    <meta property="og:description" content="UPST-NeRF: Universal Photorealistic Style Transfer of Neural Radiance Fields for 3D Scene"/>
    <link href="https://fonts.googleapis.com/css2?family=Material+Icons" rel="stylesheet">

<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-6HHDEXF452"></script>
<script>
    window.dataLayer = window.dataLayer || [];
    function gtag(){dataLayer.push(arguments);}
    gtag('js', new Date());
    gtag('config', 'G-6HHDEXF452');
</script>

</head>


 <body>
<div class="container">
    <div class="paper-title">
      <h1>UPST-NeRF: Universal Photorealistic Style Transfer of Neural Radiance Fields for 3D Scene</h1>
    </div>

    
    <div id="authors">
        <div class="author-row">
            <div class="col-5 text-center"><a href="https://scholar.google.com.hk/citations?user=HvWZhM4AAAAJ&hl=zh-CN">Yaosen Chen</a><sup>1</sup></div>
            <div class="col-5 text-center">Qi Yuan</a><sup>1</sup></div>
            <div class="col-5 text-center">Zhiqiang Li</a><sup>1</sup></div>
            <div class="col-5 text-center">Yuegen Liu</a><sup>1,3</sup></div>
            <div class="col-5 text-center"><a href="https://scholar.google.com.hk/citations?user=tfJVFEcAAAAJ&hl=zh-CN">Wei Wang</a><sup>*1,2</sup></div>
            <div class="col-5 text-center">Chaoping Xie</a><sup>1,2</sup></div>
            <div class="col-5 text-center">Xuming Wen</a><sup>1,2</sup></div>
            <div class="col-5 text-center">Qien Yu</a><sup>4</sup></div>
        </div>

        <div class="affil-row">
            <div class="col-8 text-center"><sup>1</sup>Media Intelligence Laboratory, ChengDu Sobey Digital Technology Co., Ltd</a></div>
            <div class="col-8 text-center"><sup>2</sup>Peng Cheng Laboratory</a></div>
            <div class="col-8 text-center"><sup>3</sup>Southwest Jiaotong University</div>
            <div class="col-8 text-center"><sup>4</sup>Sichuan University</div>
        </div>

        <p class="caption">
            *Corresponding Author is Wei Wang.
        </p>


        <div style="clear: both">
            <div class="paper-btn-parent">
            <a class="supp-btn" href="https://arxiv.org/pdf/2208.07059.pdf">
                <span class="material-icons"> description </span> 
                 Paper
            </a>

            <a class="supp-btn" href="https://github.com/semchan/UPST-NeRF">
                <span class="material-icons"> description </span> 
                  Code
            </a>
        </div></div>
    </div>


    <section id="paper">
        <h2>Paper</h2>
        <hr>
        <div class="flex-row">
            <div style="box-sizing: border-box; padding: 16px; margin: auto;">
                <a href="assets/upstnerf.pdf"><img class="screenshot" src="assets/paper_preview.png"></a>
            </div>
            <div style="width: 50%">
                <p><b>UPST-NeRF: Universal Photorealistic Style Transfer of Neural Radiance Fields for 3D Scene</b></p>
                <p>Yaosen Chen, Qi Yuan, Zhiqiang Li, Yuegen Liu, Wei Wang, Chaoping Xie, Xuming Wen, Qien Yu</p>

                
                <div><span class="material-icons"> description </span><a href="https://arxiv.org/abs/2208.07059"> arXiv version</a></div>

            </div>
        </div>
    </section>



    <section id="teaser" class="flex-row">
            <a href="assets/image/tear.png" style="text-align: center;">
                <img width="90%" src="assets/image/tear.png" >
            </a>
        <p class="caption">
            <strong>Transferring photorealistic style with a style image in the 3D scene.</strong>Multi-view images of a given set of 3D scenes (a) and a style image (b), our model is capable of rendering photorealistic stylized novel views  (c) with a consistent appearance at various view angles in 3D space.
        </p>
    </section>
    





    <section id="abstract"/>
        <h2>Abstract</h2>
        <hr>

        <p>3D scenes photorealistic stylization aims to generate photorealistic images from arbitrary novel views according to a given style image while ensuring consistency when rendering from different viewpoints. Some existing stylization methods with neural radiance fields can effectively predict stylized scenes by combining the features of the style image with multi-view images to train 3D scenes. However, these methods generate novel view images that contain objectionable artifacts. Besides, they cannot achieve universal photorealistic stylization for a 3D scene. Therefore, a styling image must retrain a 3D scene representation network based on a neural radiation field. We propose a novel 3D scene photorealistic style transfer framework to address these issues. It can realize photorealistic 3D scene style transfer with a 2D style image. We first pre-trained a 2D photorealistic style transfer network, which can meet the photorealistic style transfer between any given content image and style image. Then, we use voxel features to optimize a 3D scene and get the geometric representation of the scene. Finally, we jointly optimize a hyper network to realize the scene photorealistic style transfer of arbitrary style images. In the transfer stage, we use a pre-trained 2D photorealistic network to constrain the photorealistic style of different views and different style images in the 3D scene. The experimental results show that our method not only realizes the 3D photorealistic style transfer of arbitrary style images but also outperforms the existing methods in terms of visual quality and consistency. Project page:https://semchan.github.io/UPST_NeRF/.
            </p>
    </section>


    <section id="method"/>
        <h2>Approach</h2>
<hr>

    <section id="teaser" class="flex-row">
            <a href="assets/image/overview.png" style="text-align: center;">
                <img width="90%" src="assets/image/overview.png" >
            </a>
        <p class="caption">
            <strong>Overview of Universal Photorealistic Style Transfer of Neural Radiance Fields.</strong>In our framework, the training in photorealistic style transfer in 3D scenes divides into two stages. The first stage is geometric training for a single scene. We use the density voxel grid and feature voxel grid to represent the scene directly, and the density voxel grid is used to output density; the feature voxel grid with a shallow MLP of RGBNet use to predict the color. The second stage is style training. The parameters of the density voxel grid and feature voxel grid will be frozen, and we use a reference style image's features to be the input of the hyper network, which can control the RGBNet's input. Thus, we jointly optimize the hyper network to realize the scene photorealistic style transfer with arbitrary style images.
        </p>
    </section>


    <hr>
    <section id="teaser-videos">
        <div class="flex-row">
        <figure style="width: 50%;">
            <a href="assets/image/net.png">
                <img width="100%" src="assets/image/net.png">
            </a>
        </figure>
            <div style="width: 40%;">
                <br><br>
                <p> <strong>The architecture of YUVStyleNet.</strong>  We designed a framework for 2D photorealistic style transfer, which supports the input of a full resolution style image and a full resolution content image, and realizes the photorealistic transfer of styles from the style image to the content image. In this framework, we transform the image into YUV channels. The final fusion uses the generated stylized UV channel, and the Y channel fusion after the stylized image is fused with the original content image to get the final photorealistic stylized image.</p> 
            </div>
        </div>
    </section>









    <section id="results">
        <h2>Qualitative comparisons with artistic style images</h2>
        <hr>
        <section>
            <div class="flex-row">
                <figure style="width: 100%; text-align: center;">
                    <video width="100%" controls muted loop autoplay>
                        <source src="assets/NERF_dataset_upst-nerf.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </figure>
                
            </div>
        </section>
        
        <br/>
        <hr>
    </section>
    
    
    
    
    
    
    <section id="results">
        <h2> Qualitative comparisons with photorealistic style images</h2>

        <section>
            <div class="flex-row">
                <figure style="width: 100%; text-align: center;">
                    <video width="100%" controls muted loop autoplay>
                        <source src="assets/llff_dataset_upst-nerf.mp4" type="video/mp4">
                        Your browser does not support the video tag.
                    </video>
                </figure>
                
            </div>
        </section>

    </section>
    









<hr>

   <section id="Consistency">
        <h2> Consistency comparisons</h2>

    <section id="teaser" class="flex-row">
            <a href="assets/image/overview.png" style="text-align: center;">
                <img width="60%" src="assets/image/consistency.png" >
            </a>
        <p class="caption">
            <strong>Short-range and Long-range consistency.</strong>We use every two adjacent novel views ($O_{i},O_{i+1}$) and view pairs of gap 5 ($O_{i},O_{i+5}$) for short and long-range consistency calculation. The comparisons of short and long-range consistency are shown in Tab.1 and Tab.2, respectively. Our method outperforms other methods by a significant margin.
        </p>
    </section>

    </section>
    




<hr>

   <section id="UserStudy">
        <h2> User study</h2>

    <section id="teaser" class="flex-row">
            <a href="assets/image/overview.png" style="text-align: center;">
                <img width="40%" src="assets/image/userstudy.png" >
            </a>
        <p class="caption">
            <strong>User study.</strong>We record the user preference in the form of boxplot. Our results win more preferences both in the photorealistic stylization and consistency quality.
        </p>
    </section>

    </section>





    





</code></pre>
    </section>

<br />
    

    <section id="appendix">
        <h2>Appendix</h2>
        <hr>
        <h4>on NeRF-Synthetic datasets</h4>
        <hr>
        <div class="flex-row">
            <figure style="width: 100%;height: 100%;text-align: center;">
                <video class="left" width="100%" height="100%" controls muted loop autoplay>
                    <source src="assets/nerf_dataset_upst-nerf-more1.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <video class="left" width="100%" height="100%" controls muted loop autoplay>
                    <source src="assets/nerf_dataset_upst-nerf-more2.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>


            </figure>
        </div>
        <h4>on Local Light Field Fusion(LLFF) datasets</h4>
        <hr>
        <div class="flex-row">
            <figure style="width: 100%;height: 100%;text-align: center;">
                <video class="left" width="100%" height="100%" controls muted loop autoplay>
                    <source src="assets/llff_dataset_upst-nerf-more1.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>
                <video class="left" width="100%" height="100%" controls muted loop autoplay>
                    <source src="assets/llff_dataset_upst-nerf-more2.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video>

            </figure>
        </div>          
    </section>



    <section id="bibtex">
        <h2>Citation</h2>
        <hr>
        <pre><code>
        @inproceedings{chen2022upstnerf,
        title = {UPST-NeRF: Universal Photorealistic Style Transfer of Neural Radiance Fields for 3D Scene},
        author = {Yaosen Chen and Qi Yuan and Zhiqiang Li and Yuegen Liu and Wei Wang and Chaoping Xie and Xuming Wen and Qien Yu},
        year = {2022},
        booktitle = {arxiv}
        }



</code></pre>
    </section>
</div>
</body>
</html>
